# Сравнение результатов моделей для улучшения Directional Accuracy

## Результаты экспериментов

### 1. Улучшенная LSTM модель (baseline)
- **Test DA**: 46-64% (в зависимости от запуска)
- **Количество параметров**: ~3.2M
- **Преимущества**: Стабильность, быстрое обучение
- **Недостатки**: Ограниченная способность к захвату долгосрочных зависимостей

### 2. Transformer модель
- **Test DA**: 43.4%
- **Validation DA**: до 61.9%
- **Количество параметров**: ~4.5M
- **Преимущества**: Отличное понимание контекста, параллельная обработка
- **Недостатки**: Склонность к переобучению, дисбаланс классов

### 3. CNN+LSTM модель (новая архитектура)
- **Test DA**: 56.6%
- **Validation DA**: 62.9%
- **Количество параметров**: ~4.8M
- **Преимущества**:
  - Лучшее извлечение локальных паттернов через CNN
  - Эффективное моделирование временных зависимостей через LSTM
  - Баланс между производительностью и выразительной силой
- **Недостатки**:
  - Некоторый сдвиг в предсказаниях (100% точность для роста, 0% для падения)

## Анализ результатов

### Ключевые находки:
1. **CNN+LSTM показала лучший результат** среди всех протестированных архитектур
2. **Transformer архитектура** показала высокий потенциал (61.9% на валидации), но страдает от переобучения
3. **Современные гибридные архитектуры** превосходят классическую LSTM

### Потенциальные причины сдвига в CNN+LSTM:
- Дисбаланс классов в обучающей выборке
- Функция потерь может быть неоптимальной для балансировки классов
- Необходима стратегия oversampling/undersampling

## Рекомендации по дальнейшему улучшению DA

### 1. Балансировка данных
```python
# Использовать стратегии балансировки:
- Oversampling дней с падением цены
- Undersampling дней с ростом цены
- SMOTE для синтетического расширения редких классов
```

### 2. Взвешенная функция потерь
```python
# Увеличить вес для дней с падением цены
class BalancedLoss(nn.Module):
    def __init__(self):
        super().__init__()
        self.up_weight = 1.0    # стандартный вес для роста
        self.down_weight = 2.0  # повышенный вес для падения
```

### 3. Ансамбли моделей
- Комбинировать предсказания Transformer и CNN+LSTM моделей
- Потенциальное улучшение DA на 5-10%

### 4. Дополнительные улучшения CNN+LSTM
- Добавить attention mechanism между CNN и LSTM
- Использовать более сложные архитектуры для извлечения паттернов
- Ввести регуляризацию для балансировки предсказаний

## Выводы

1. **CNN+LSTM архитектура** показала лучший результат с DA 56.6%
2. **Целевой показатель DA > 65%** остается достижимым с дополнительной оптимизацией
3. **Гибридные архитектуры** являются наиболее перспективным направлением
4. **Проблема дисбаланса классов** требует отдельного внимания

## Следующие шаги

1. Реализовать балансировку данных в CNN+LSTM модели
2. Попробовать ансамбль из Transformer и CNN+LSTM
3. Добавить более сложные методы регуляризации
4. Провести дополнительное тестирование на разных временных периодах
