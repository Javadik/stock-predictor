# Рекомендации по дальнейшему улучшению Directional Accuracy

## Анализ текущих результатов

После реализации и тестирования нескольких версий Transformer архитектуры были получены следующие результаты:

| Модель | Тестовая DA | Лучшая валидационная DA |
|--------|-------------|------------------------|
| Оригинальная LSTM | 43-59% | 43-59% |
| Улучшенный Transformer | 43.4% | 61.9% |
| Продвинутый Transformer | 42.4% | 61.9% |
| Финальный Transformer | 43.4% | 60.8% |

## Ключевые наблюдения

### 1. Переобучение
- Валидационная DA значительно выше тестовой DA
- Это указывает на переобучение модели
- Модель достигает DA до 61.9% на валидационных данных, но только 42-43% на тестовых

### 2. Дисбаланс предсказаний
- Модели склонны предсказывать падение цен
- В последних экспериментах точность для падения составляет 88-100%, а для роста - 0-7%
- Это указывает на дисбаланс в данных или функции потерь

### 3. Потенциал архитектуры
- Валидационная DA до 61.9% показывает, что архитектура способна на высокую точность
- Это близко к целевым 65%, что указывает на правильность выбранного направления

## Рекомендации по улучшению DA

### 1. Балансировка данных
```python
# Рекомендуется использовать стратегии балансировки:
- Oversampling редких классов (дни с ростом)
- Undersampling частых классов (дни с падением)
- SMOTE для синтетического расширения данных
- Взвешивание потерь в зависимости от класса
```

### 2. Улучшенная функция потерь
```python
# Вместо текущей функции потерь:
class BalancedDirectionalLoss(nn.Module):
    def __init__(self):
        super().__init__()
        self.mse_loss = nn.MSELoss()
        # Взвешивание потерь для разных направлений
        self.up_weight = 1.5  # Увеличенный вес для дней с ростом
        self.down_weight = 1.0  # Стандартный вес для дней с падением

    def forward(self, pred, target):
        # Разделение на рост и падение
        up_mask = target > 0
        down_mask = target < 0

        # Расчет потерь для каждого направления с разными весами
        up_loss = self.mse_loss(pred[up_mask], target[up_mask]) * self.up_weight if up_mask.any() else 0
        down_loss = self.mse_loss(pred[down_mask], target[down_mask]) * self.down_weight if down_mask.any() else 0

        return (up_loss + down_loss) / 2
```

### 3. Аугментация данных
```python
# Техники аугментации для временных рядов:
- Временной сдвиг (time warping)
- Добавление шума (gaussian noise)
- Инверсия (inversion)
- Масштабирование (scaling)
```

### 4. Регуляризация
```python
# Усилить регуляризацию для предотвращения переобучения:
- Увеличить dropout до 0.3-0.4
- Добавить L2 регуляризацию
- Использовать dropout на выходе из attention слоев
- Применить batch normalization
```

### 5. Улучшенная архитектура
```python
# Рекомендуемые улучшения:
- Добавить residual connections
- Использовать layer normalization
- Применить gradient clipping
- Внедрить learning rate scheduling
```

### 6. Кросс-валидация
```python
# Использовать временные окна для кросс-валидации:
- Walk-forward validation
- Time series cross-validation
- Rolling window validation
```

## Потенциальные следующие шаги

### 1. Гибридная архитектура CNN+LSTM
- Реализовать CNN+LSTM архитектуру как альтернативу
- CNN для извлечения локальных паттернов, LSTM для временных зависимостей
- Потенциал: 50-65% DA на основе анализа

### 2. Ансамбли моделей
- Комбинировать предсказания нескольких моделей
- Взвешенное голосование
- Потенциал: +5-10% к лучшей отдельной модели

### 3. Модель с вниманием к рыночным условиям
- Добавить внешние признаки (VIX, экономические индикаторы)
- Использовать attention для фокусировки на важных периодах
- Адаптивная модель в зависимости от рыночной волатильности

## Ожидаемые улучшения

С реализацией вышеуказанных рекомендаций:
- Краткосрочная цель: 5-60% DA (в ближайшие 2-3 эксперимента)
- Среднесрочная цель: 65% DA (с гибридными архитектурами)
- Долгосрочная цель: 70%+ DA (с ансамблями и внешними данными)

## Выводы

1. Transformer архитектура показала значительный потенциал с валидационной DA до 61.9%
2. Основные проблемы: переобучение и дисбаланс классов
3. С учетом рекомендаций по балансировке данных и регуляризации, достижение 65% DA вполне реально
4. Следующим шагом рекомендуется реализовать CNN+LSTM архитектуру и сравнить с Transformer
