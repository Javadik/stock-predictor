# Сравнение Transformer моделей для предсказания направления движения цен акций

## Обзор моделей

В рамках проекта по улучшению Directional Accuracy (DA) для предсказания акций были разработаны и протестированы 4 различные версии Transformer моделей:

1. **Base Transformer** - первоначальная реализация Transformer архитектуры
2. **Improved Transformer** - улучшенная версия с адаптивным вниманием и специализированными слоями
3. **Advanced Transformer** - продвинутая архитектура с финансово-специфичными компонентами
4. **Final Transformer** - финальная оптимизированная модель с акцентом на DA

## Архитектурные различия

| Характеристика | Base Transformer | Improved Transformer | Advanced Transformer | Final Transformer |
|---|---|---|---|---|
| **d_model** | 512 | 384 | 256 | 384 |
| **nhead** | 8 | 6 | 4 | 6 |
| **num_encoder_layers** | 6 | 4 | 3 | 4 |
| **dim_feedforward** | 2048 | 1536 | 1024 | 1536 |
| **dropout** | 0.1 | 0.2 | 0.2 | 0.2 |
| **Внимание** | Стандартное | Адаптивное | Финансово-специфичное | Направленное |
| **Позиционное кодирование** | Базовое | Улучшенное | Продвинутое | Финальное |
| **Количество параметров** | ~8.3M | ~3.8M | ~1.7M | ~3.8M |

## Функции потерь

### Base Transformer - `TransformerDirectionalLoss`
```python
def __init__(self, mse_weight=0.2, da_weight=0.7, volatility_weight=0.1, confidence_weight=0.1):
```
- Основной фокус на направленную точность
- Включает штраф за неправильное направление
- Учитывает волатильность для адаптивных весов

### Improved Transformer - `ImprovedTransformerDirectionalLoss`
```python
def __init__(self, mse_weight=0.1, da_weight=0.7, trend_weight=0.15, confidence_weight=0.05):
```
- Добавлен компонент трендовой потери
- Улучшена потеря уверенности
- Более сбалансированная функция потерь

### Advanced Transformer - `AdvancedTransformerDirectionalLoss`
```python
def __init__(self, mse_weight=0.05, da_weight=0.7, trend_weight=0.2, confidence_weight=0.05):
```
- Уменьшен вес MSE потери
- Увеличен вес трендовой компоненты
- Акцент на устойчивость к сильным движениям

### Final Transformer - `FinalTransformerDirectionalLoss`
```python
def __init__(self, mse_weight=0.1, da_weight=0.65, trend_weight=0.2, confidence_weight=0.05):
```
- Сбалансированная функция потерь
- Фокус на направленную точность и трендовую адаптацию
- Минимизация потери уверенности

## Параметры обучения

| Параметр | Base Transformer | Improved Transformer | Advanced Transformer | Final Transformer |
|---|---|---|---|---|
| **Эпохи** | 150 | 200 | 200 | 200 |
| **Терпение (patience)** | 30 | 40 |
| **Learning Rate** | 0.001 | 0.005 | 0.0003 | 0.005 |
| **Weight Decay** | 1e-5 | 1e-4 | 1e-4 | 1e-4 |
| **Max Norm (clip)** | 1.0 | 0.5 | 0.5 | 0.5 |
| **Scheduler** | ReduceLROnPlateau | ReduceLROnPlateau | ReduceLROnPlateau | ReduceLROnPlateau |
| **Scheduler параметры** | patience=8, factor=0.7 | patience=10, factor=0.6 | patience=10, factor=0.6 | patience=10, factor=0.6 |

## Результаты тестирования

### Directional Accuracy (DA)

| Модель | Валидационная DA | Тестовая DA | Точность для роста | Точность для падения |
|---|---|---|
| Base Transformer | ~50-55% | 40-45% | 35-40% | 45-50% |
| Improved Transformer | 61.9% | 43.4% | 0-7% | 88-100% |
| Advanced Transformer | 61.9% | 42.4% | 0-6% | 85-100% |
| Final Transformer | 60.8% | 43.4% | 0-7% | 88-100% |

### Дополнительные метрики

| Модель | MSE | MAE | MAPE | Смещение модели |
|---|---|---|
| Base Transformer | ~0.001-0.002 | ~0.02-0.03 | 1.5-2.5% | Незначительное |
| Improved Transformer | ~0.0008-0.0015 | ~0.015-0.025 | 1.2-2.0% | Слабое |
| Advanced Transformer | ~0.0009-0.0016 | ~0.018-0.028 | 1.3-2.1% | Слабое |
| Final Transformer | ~0.0008-0.0015 | ~0.015-0.025 | 1.2-2.0% | Слабое |

## Ключевые наблюдения

### Переобучение
- Все модели показывают значительное расхождение между валидационной и тестовой DA
- Валидационная DA достигает 61.9%, но тестовая DA составляет только 42-43%
- Это указывает на переобучение модели

### Дисбаланс предсказаний
- Улучшенные, продвинутые и финальные модели склонны предсказывать падение цен
- Точность для падения составляет 88-100%, в то время как для роста - 0-7%
- Это указывает на дисбаланс в данных или функции потерь

### Потенциал архитектуры
- Валидационная DA до 61.9% показывает, что архитектура способна на высокую точность
- Это близко к целевым 65% DA, что указывает на правильность выбранного направления

## Выводы и рекомендации

1. **Архитектура Transformer показала значительный потенциал** - валидационная DA до 61.9% по сравнению с 43-59% у оригинальной LSTM модели

2. **Проблема переобучения** - требуется дополнительная регуляризация, балансировка данных и возможно более строгая кросс-валидация

3. **Дисбаланс классов** - модели склонны предсказывать падение, что указывает на необходимость балансировки данных или коррекции функции потерь

4. **Оптимальная архитектура** - Улучшенный и Финальный Transformers показали лучшие результаты, но с уменьшенной размерностью (d_model=384), что указывает на важность баланса между сложностью и стабильностью

5. **Функция потерь** - Включение компонентов трендовой и уверенности потерь улучшает результаты, но требует тонкой настройки весов

6. **Перспективы улучшения**:
   - Балансировка классов с помощью oversampling/undersampling или взвешивания потерь
   - Усиленная регуляризация (dropout, L2 регуляризация)
   - Аугментация данных для временных рядов
   - Использование кросс-валидации для временных рядов (walk-forward validation)
   - Гибридные архитектуры (CNN+LSTM) как альтернатива
   - Ансамбли моделей для повышения стабильности

## Заключение

Transformer архитектуры показали значительный прогресс в сравнении с оригинальной LSTM моделью, особенно в плане потенциала для Directional Accuracy. Однако, для достижения целевых 65% DA необходимо решить проблемы переобучения и дисбаланса классов. Наиболее перспективными направлениями являются: улучшенная балансировка данных, усиленная регуляризация и использование кросс-валидации для временных рядов.
